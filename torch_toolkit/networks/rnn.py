"""Various recurrent layers and utilitiyes"""

__all__ = ['break_grad', 'mask_state', 'update_state_with_index', 'NormGRUCell', 'update_state_with_mask',
           'SequentialStartState', 'SequentialPassState', 'SequentialEndState', 'FakeNormGRUCell']
from typing import Optional, Tuple, Union

import torch
import torch as th
import torch.nn as nn

from .activations import Tanh
from .init import layer_init
from .normalization import RMSNorm
from ..typing import Tensor

State = Union[Tensor, Tuple[Tensor, Tensor]]


def break_grad(x: Tensor) -> Tensor:
    """Detach and reattach gradient (e.g., across batch boundaries)

    Args:
        x: Input tensor
    Returns:
        Tensor without previous gradient history
    """
    return x.detach_().requires_grad_()


def mask_state(state_original: Tensor, reset: Tensor, initial_state: Tensor) -> Tensor:
    """Replace state with initial state where reset

    Args:
        state_original: [Bxh] incoming state
        reset: [B] float or bool tensor (1 where reset)
        initial_state: [B/1xh] initial state to use where we reset
    Returns:
        modified state
    """
    rs = reset.float().unsqueeze(-1)  # Convert to float, unqueeze batch dim
    return state_original * (1. - rs) + initial_state.expand_as(state_original) * rs


def update_state_with_mask(state_original: Tensor, update: Tensor, new_state: Tensor) -> Tensor:
    """Update state with new state where updating. Allows us to update different elements of state batch at varying times

    Args:
        state_original: [Bxh] incoming state
        update: [B] mask which is >0 where we're updating state on this timestep
        new_state: [<B] new state as generated by gru(x[update], state_original[update])
    Returns:
        new_state. Uses original state where we aren't updating, uses new state where we are. Preserves gradient
    """
    s = th.zeros_like(state_original)
    s = th.masked_scatter(s, (update > 0).unsqueeze(-1), new_state)
    return mask_state(state_original, update.float(), s)


def update_state_with_index(state: Tensor, tidx: Tensor, ntidx: Tensor, idx_state: Tensor) -> Tensor:
    """Update state at idx with idx_state, otherwise return state

    Args:
        state: Original state
        tidx: bool or long tensor with indices where we update state
        ntidx: bool or long tensor with indices where we don't update state
        idx_state: [ntidx.shape[0]xh] Updated state
    Returns:
        state updated where it needs to be
    """
    s = th.zeros_like(state)
    s[tidx] = idx_state
    s[ntidx] = state[ntidx]
    return s


class SequentialPassState(nn.Sequential):
    """Sequential Module that takes additional input that it ignores"""
    def forward(self, x, state: State) -> Tuple[Tensor, State]:
        for module in self:
            x = module(x)
        return x, state


class SequentialStartState(nn.Sequential):
    """Sequential module that takes additional input only relevant to first module"""
    def forward(self, x, state: State) -> Tuple[Tensor, State]:
        for i, module in enumerate(self):
            if i == 0: x, state = module(x, state)
            else: x = module(x)
        return x, state


class SequentialEndState(nn.Sequential):
    """Sequential module that takes additional input only relevant to last module"""
    def forward(self, x, state: State) -> Tuple[Tensor, State]:
        for i, module in enumerate(self):
            if i == len(self) - 1: x, state = module(x, state)
            else: x = module(x)
        return x, state


class ResetCore(nn.Module):
    """Clone of haiku's hk.ResetCore with more features

    Reset incoming state based on "should_reset" signal

    Args:
        rnn: RNN module
        learning_dim: If 0, typical zero-reset. If 1, single learnable state. If >1, multiple learnable states (i.e., per-option)
    """
    __constants__ = ['ldim', 'hidden_size']
    hidden_size: torch.jit.Final[int]
    ldim: torch.jit.Final[int]

    def __init__(self, rnn: nn.Module, learning_dim: int):
        super().__init__()
        assert learning_dim >= 0
        self.rnn = rnn
        self.hidden_size = getattr(rnn, 'hidden_size', 0)  # Recurrent modules have hidden size, others do not
        if not self.hidden_size: learning_dim = 0
        self.ldim = learning_dim
        self._initial_state = nn.Parameter(torch.zeros(self.ldim, self.hidden_size), requires_grad=bool(learning_dim))

    def forward(self, state, should_reset, reset_indices: Optional[torch.LongTensor] = None):
        """"""
        if self.ldim <= 1: return mask_state(state, should_reset, self._initial_state)
        else:
            if reset_indices is None: reset_indices = torch.zeros_like(should_reset, dtype=torch.int64)
            return mask_state(state, should_reset, self.initial_state_index(reset_indices))

    def initial_state(self, b: int = 1):
        return self._initial_state.expand(b, -1)

    def initial_state_index(self, indices: torch.LongTensor):
        return self._initial_state[indices]


def str_to_norm_cls(norm_type: str = 'rms'):
    norm_cls = RMSNorm if norm_type == 'rms' else nn.LayerNorm if norm_type == 'ln' else nn.Identity
    return norm_cls


class FakeNormGRUCell(nn.Module):
    """Fake GRU class. Just a linear layer with tanh, optional laayernorm

    Args:
        input_size: Number of input units. Remember to include dimension of action if providing
        hidden_size: Number of output units, also size of hidden state
        bias: Use a bias
        n_preact: LayerNorm before any activation
        norm_type: One of 'ln, rms' or anything else. ln for LayerNorm, rms for RMSNorm, anything else for none (Identity)
    """
    def __init__(self, input_size: int, hidden_size: int, bias: bool = True, n_preact: bool = True, norm_type: str = 'rms'):
        super().__init__()
        self.core = nn.Sequential(layer_init(nn.Linear(input_size, hidden_size, bias=bias)),  str_to_norm_cls(norm_type)(hidden_size), Tanh(inplace=True))

    def forward(self, input: Tensor, hx: Tensor):
        return self.core(input)


class NormGRUCell(nn.RNNCellBase):
    n_preact: torch.jit.Final[bool]
    """Layer/RMS-normalized GRU as in https://arxiv.org/pdf/1607.06450.pdf

    https://github.com/pytorch/pytorch/issues/12482#issuecomment-440485163
    
    Args:
        input_size: Number of input units. Remember to include dimension of action if providing
        hidden_size: Number of output units, also size of hidden state
        bias: Use a bias
        n_preact: LayerNorm before any activation
        norm_type: One of 'ln, rms' or anything else. ln for LayerNorm, rms for RMSNorm, anything else for none (Identity)
    """
    def __init__(self, input_size, hidden_size, bias=True, n_preact=True, norm_type: str = 'rms'):
        super().__init__(input_size, hidden_size, bias, num_chunks=3)
        norm_cls = RMSNorm if norm_type == 'rms' else nn.LayerNorm if norm_type == 'ln' else nn.Identity
        self.n_preact = n_preact
        if n_preact:
            self.n_ih = norm_cls(3 * self.hidden_size)
            self.n_hh = norm_cls(3 * self.hidden_size)
        self.n_in = norm_cls(self.hidden_size)
        self.n_hn = norm_cls(self.hidden_size)
        # Orthogonal initialization
        nn.init.orthogonal_(self.weight_hh, 2 ** 0.5)
        nn.init.orthogonal_(self.weight_ih, 2 ** 0.5)
        if self.bias:
            nn.init.constant_(self.bias_hh, 0)
            nn.init.constant_(self.bias_ih, 0)

    def forward(self, input: Tensor, hx: Tensor):
        ih = input @ self.weight_ih.t() + self.bias_ih
        hh = hx @ self.weight_hh.t() + self.bias_hh
        if self.n_preact:
            ih = self.n_ih(ih)
            hh = self.n_hh(hh)

        i_r, i_z, i_n = ih.chunk(3, dim=1)
        h_r, h_z, h_n = hh.chunk(3, dim=1)
        i_n = self.n_in(i_n)
        h_n = self.n_hn(h_n)

        r = th.sigmoid(i_r + h_r)
        z = th.sigmoid(i_z + h_z)
        n = th.tanh(i_n + r * h_n)
        h = (1 - z) * n + z * hx
        return h

